{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJRIeRBap_sO"
      },
      "source": [
        "Run the cell below to finetune the pretrained model to suit your dataset.\n",
        "<br>\n",
        "<br>\n",
        "Once done, download best_model.pth and classifications.txt, this is the model that has been finetuned on your data and the cell names that model has learnt.\n",
        "<br>\n",
        "<br>\n",
        "Proceed to bottom to use cell identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aFX6mScSnprv"
      },
      "outputs": [],
      "source": [
        "import shutil, torch, timm, os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def create_folder(foldername):\n",
        "  try:\n",
        "    os.mkdir(foldername)\n",
        "\n",
        "  except FileExistsError:\n",
        "    print(f\"{foldername} already exists.\")\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "def split_new_data():\n",
        "  source_directory = \"/content/drive/MyDrive/MNT/Dataset\"\n",
        "\n",
        "  try:\n",
        "    folders = os.listdir(source_directory) # access new data if there is any\n",
        "\n",
        "    for folder in folders: # access each new cell type added\n",
        "      folder_path = os.path.join(source_directory, folder)\n",
        "      files = os.listdir(folder_path)\n",
        "\n",
        "      validate_path = os.path.join(\"/content/drive/MyDrive/MNT/Training Dataset/Validate\", folder)\n",
        "      training_path = os.path.join(\"/content/drive/MyDrive/MNT/Training Dataset/Train\", folder)\n",
        "\n",
        "      create_folder(validate_path)\n",
        "      create_folder(training_path)\n",
        "\n",
        "      size = len(files)\n",
        "      train = (size * 8) // 10 # 80% of cell images will be used for training, remaining for validation\n",
        "\n",
        "      for f in files[:train]:\n",
        "        current_path = os.path.join(folder_path, f)\n",
        "        new_path = os.path.join(training_path, f)\n",
        "        shutil.move(current_path, new_path)\n",
        "\n",
        "      for f in files[train:]:\n",
        "        current_path = os.path.join(folder_path, f)\n",
        "        new_path = os.path.join(validate_path, f)\n",
        "        shutil.move(current_path, new_path)\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(\"No new data found.\")\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "def train_epoch(loader, model, criterion, optimizer, device):\n",
        "  model.train()\n",
        "  total_loss, total_correct = 0.0, 0\n",
        "\n",
        "  for imgs, labels in loader:\n",
        "      imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(imgs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item() * imgs.size(0)\n",
        "      total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "  avg_loss = total_loss / len(loader.dataset)\n",
        "  avg_acc = total_correct / len(loader.dataset)\n",
        "\n",
        "  return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "def eval_epoch(loader, model, criterion, device):\n",
        "  model.eval()\n",
        "  total_loss, total_correct = 0.0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "          imgs, labels = imgs.to(device), labels.to(device)\n",
        "          outputs = model(imgs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          total_loss += loss.item() * imgs.size(0)\n",
        "          total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "  avg_loss = total_loss / len(loader.dataset)\n",
        "  avg_acc = total_correct / len(loader.dataset)\n",
        "  return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "def finetune(num_epochs):\n",
        "  # Imagenet normalisation statistics\n",
        "  mean = [0.485, 0.456, 0.406]\n",
        "  std = [0.229, 0.224, 0.225]\n",
        "\n",
        "  # makes dataset larger\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.RandomResizedCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean, std),\n",
        "  ])\n",
        "\n",
        "  # crop images to 224x224px for deep learning\n",
        "  val_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean, std),\n",
        "  ])\n",
        "\n",
        "  # access training dataset\n",
        "  train_ds = datasets.ImageFolder(\"/content/drive/MyDrive/MNT/Training Dataset/Train\", transform=train_transform)\n",
        "  val_ds = datasets.ImageFolder(\"/content/drive/MyDrive/MNT/Training Dataset/Validate\", transform=val_transform)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=4)\n",
        "  val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "  num_classes = len(train_ds.classes)\n",
        "\n",
        "  # load in EfficientNet-LiteB0 pretrained model\n",
        "  model = timm.create_model('efficientnet_lite0', pretrained=True)\n",
        "\n",
        "  # replace classifications\n",
        "  in_features = model.classifier.in_features\n",
        "  model.classifier = nn.Linear(in_features, num_classes)\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "  best_val_acc = 0.0\n",
        "  patience = 10\n",
        "  no_improvement_epochs = 0\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
        "      train_loss, train_acc = train_epoch(train_loader, model, criterion, optimizer, device)\n",
        "\n",
        "      val_loss, val_acc = eval_epoch(val_loader, model, criterion, device)\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "      if val_acc > best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          no_improvement_epochs = 0\n",
        "          torch.save(model.state_dict(), 'best_model.pth') # save best model\n",
        "      else:\n",
        "          no_improvement_epochs += 1\n",
        "\n",
        "      if no_improvement_epochs >= patience: # early exit to prevent overfitting\n",
        "          print(f\"Early stopping at epoch {epoch+1}. No improvement in the last {patience} epochs.\")\n",
        "          break\n",
        "\n",
        "  with open(\"classifications.txt\", \"w\") as file: # write classifications file\n",
        "    file.write(train_ds.classes[0])\n",
        "    for Class in train_ds.classes[1:]:\n",
        "        file.write(f\"\\n{Class}\")\n",
        "\n",
        "  print(\"Finished finetuning.\")\n",
        "\n",
        "  return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "  split_new_data() # place new data in dataset in training and validation if there is any\n",
        "\n",
        "  finetune(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31fym-PRMI00"
      },
      "source": [
        "Cell Identifier.\n",
        "<br>\n",
        "<br>\n",
        "Ensure classifications.txt and best_model.pth has been uploaded to files under content."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cellpose"
      ],
      "metadata": {
        "id": "OFyN1Kka9Sho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vHPbpRfgJ3GT"
      },
      "outputs": [],
      "source": [
        "import torch, timm, cv2\n",
        "import cellpose\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from cellpose import models\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  with open(\"classifications.txt\", \"r\") as file:\n",
        "      classes = [line.strip() for line in file]\n",
        "  num_classes = len(classes)\n",
        "\n",
        "  cellpose_model = models.CellposeModel(gpu=\"True\")\n",
        "  identifier_model = timm.create_model('efficientnet_lite0', pretrained=False)\n",
        "\n",
        "  in_features = identifier_model.classifier.in_features\n",
        "  identifier_model.classifier = torch.nn.Linear(in_features, num_classes)\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  finetuned_model = torch.load('best_model.pth', map_location=device)\n",
        "  identifier_model.load_state_dict(finetuned_model)\n",
        "  identifier_model = identifier_model.to(device)\n",
        "  identifier_model.eval()\n",
        "\n",
        "  mean = [0.485, 0.456, 0.406]\n",
        "  std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "  val_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean, std),\n",
        "  ])\n",
        "\n",
        "  while True:\n",
        "    image = Image.open(input(\"Enter image path here: \")).convert(\"RGB\") # pytorch reads PIL images\n",
        "    try:\n",
        "      input_tensor = val_transform(image).unsqueeze(0).to(device)\n",
        "      break\n",
        "\n",
        "    except FileNotFoundError:\n",
        "      print(\"Image not found.\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "      logits = identifier_model(input_tensor)\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      pred_class = logits.argmax(dim=1).item()\n",
        "      pred_conf = probs[0, pred_class].item()\n",
        "\n",
        "  image = np.array(image)\n",
        "  masks, flows, styles = cellpose_model.eval(image) # cellpose reads numpy arrays\n",
        "  cell_count = len(np.unique(masks)) - 1\n",
        "\n",
        "  print(f\"Predicted classification: {classes[pred_class]}  (confidence: {pred_conf:.2%}) (cell count: {cell_count})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}