{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5c631d",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1f82e",
   "metadata": {},
   "source": [
    "This is where you can find the workflow of how we trained out Cell Identifer Model.\n",
    "\n",
    "It begins with looking for data online, which is saved in Structures.\n",
    "\n",
    "We then use Cellpose-SAM[1] to segment individual cells within these images into .png files, which are saved in Cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372c623",
   "metadata": {},
   "source": [
    "## **Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253a05c",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007f868",
   "metadata": {},
   "source": [
    "We begin by installing all required libraries from requirements.txt, which can be seen in the main file directory. In order to speed up image processing, CUDA is installed, which allows us to make use of the GPU to greatly decreases run time. Ensure you have the latest version of python installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a1b958",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.22.1+cu118)\n",
      "Requirement already satisfied: cellpose==4.0.5 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (4.0.5)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: fastremap==1.17.1 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (1.17.1)\n",
      "Requirement already satisfied: filelock==3.18.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (3.18.0)\n",
      "Requirement already satisfied: fill_voids==2.1.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: fsspec==2025.5.1 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (2025.5.1)\n",
      "Requirement already satisfied: imagecodecs==2025.3.30 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2025.3.30)\n",
      "Requirement already satisfied: imageio==2.37.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.37.0)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (3.1.6)\n",
      "Requirement already satisfied: lazy_loader==0.4 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (0.4)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (3.0.2)\n",
      "Requirement already satisfied: mpmath==1.3.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: natsort==8.4.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (8.4.0)\n",
      "Requirement already satisfied: networkx==3.5 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (3.5)\n",
      "Requirement already satisfied: numpy==2.3.1 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (2.3.1)\n",
      "Requirement already satisfied: opencv-python-headless==4.11.0.86 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (4.11.0.86)\n",
      "Requirement already satisfied: packaging==25.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (25.0)\n",
      "Requirement already satisfied: pillow==11.2.1 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (11.2.1)\n",
      "Requirement already satisfied: roifile==2025.5.10 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (2025.5.10)\n",
      "Requirement already satisfied: scikit-image==0.25.2 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (0.25.2)\n",
      "Requirement already satisfied: scipy==1.16.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 24)) (1.16.0)\n",
      "Requirement already satisfied: segment-anything==1.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (1.0)\n",
      "Requirement already satisfied: setuptools==80.9.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.14.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (1.14.0)\n",
      "Requirement already satisfied: tifffile==2025.6.11 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 28)) (2025.6.11)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions==4.14.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (4.14.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 31)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 31)) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 31)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 31)) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 31)) (3.0.15)\n",
      "Requirement already satisfied: decorator in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ray\\desktop\\making and tinkering\\data processing\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 31)) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f78a0",
   "metadata": {},
   "source": [
    "If not already created, this code will create all necessary folders. Please run this for the other cells to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7aad6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/Structures already exists.\n",
      "./Data/Train already exists.\n",
      "./Data/Validate already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_folder(foldername):\n",
    "    try:\n",
    "        os.mkdir(foldername)\n",
    "    except FileExistsError:\n",
    "        print(f\"{foldername} already exists.\")\n",
    "\n",
    "folders = [\"./Data/Structures\", \"./Data/Train\", \"./Data/Validate\"]\n",
    "\n",
    "for folder in folders:\n",
    "    create_folder(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b7cb2",
   "metadata": {},
   "source": [
    "### Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac08f8",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739e5504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Welcome to CellposeSAM, cellpose v\n",
      "cellpose version: \t4.0.5 \n",
      "platform:       \twin32 \n",
      "python version: \t3.13.5 \n",
      "torch version:  \t2.7.1+cu118! The neural network component of\n",
      "CPSAM is much larger than in previous versions and CPU excution is slow. \n",
      "We encourage users to use GPU/MPS if available. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cellpose\n",
    "from cellpose import io, models\n",
    "import numpy as np\n",
    "from skimage import color, util, measure\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554cebb7",
   "metadata": {},
   "source": [
    "Initialise Cellpose model (Cellpose-SAM) to create masks for our images. Change the value of \"gpu=True\" to False if you do not have a Nivida GPU in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa98b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.CellposeModel(gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e934a",
   "metadata": {},
   "source": [
    "Image processing function. In order to shorten processing time, maxcells is set to 2000. This is assuming each image file provided will at least have 1 cell in the picture, thus producing 2000 cell images at the minimum to be fed to our machine learning model. It can be set higher for other purposes. See code for more in-depth explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatecellcrops(folderpath, cellname):\n",
    "    newfolderpath = f\"./Data/Train/{cellname}_Cell\"\n",
    "\n",
    "    try:\n",
    "        os.mkdir(newfolderpath)\n",
    "\n",
    "        filenames = os.listdir(folderpath)\n",
    "        maxcells = 2000\n",
    "        pbar = tqdm(total=maxcells, desc=f\"Processing {cellname}\")\n",
    "\n",
    "        cellnumber = 1\n",
    "        for file in filenames:\n",
    "            if cellnumber > maxcells:\n",
    "                break\n",
    "\n",
    "            imagepath = os.path.join(folderpath, file)\n",
    "            image = io.imread_2D(imagepath) # read image file\n",
    "            image_gray = color.rgb2gray(image) # gray scale image for easier masking\n",
    "            inp = (image_gray * 255).astype(np.uint8) # convert from floating-point to 8-bit image\n",
    "            \n",
    "            masks, flows, styles = model.eval(inp, invert=True) # mask image, invert is set to True assuming the image data provided is brightfield microscopy\n",
    "\n",
    "            for prop in measure.regionprops(masks, intensity_image=image_gray): # get properties of mask  \n",
    "                if prop.area < 50 or prop.mean_intensity < 0.1: # remove mask noise pickup from poorly prepared cell samples\n",
    "                    continue\n",
    "\n",
    "                label = prop.label\n",
    "                minr, minc, maxr, maxc = prop.bbox # top-left and bottom-right of bounding box\n",
    "\n",
    "                cell_mask = (masks == label) # True when pixels are in current region\n",
    "\n",
    "                cell_image = np.zeros_like(image) # copy the shape of the original image\n",
    "                \n",
    "                if image.ndim == 2: # if image is gray scale\n",
    "                    cell_image[cell_mask] = image[cell_mask] # copy over the pixels that are inside the region\n",
    "\n",
    "                else: # if image has color\n",
    "                    for c in range(image.shape[2]):\n",
    "                        channel = image[...,c]\n",
    "                        channel_out = np.zeros_like(channel)\n",
    "                        channel_out[cell_mask] = channel[cell_mask]\n",
    "                        cell_image[...,c] = channel_out\n",
    "\n",
    "                cell_crop = cell_image[minr:maxr, minc:maxc] # crop to bounding box\n",
    "\n",
    "                if cell_crop.dtype != np.uint8: # convert to 8-bit image to save as .png\n",
    "                    cell_crop = util.img_as_ubyte(cell_crop)\n",
    "\n",
    "                if not np.any(cell_crop):\n",
    "                    continue\n",
    "\n",
    "                cellimagepath = os.path.join(newfolderpath, f\"{cellname}_Cell_{cellnumber:04d}.png\")\n",
    "                io.imsave(cellimagepath, cell_crop)\n",
    "                cellnumber += 1\n",
    "                pbar.update(1)\n",
    "            \n",
    "        pbar.close()\n",
    "        print(f\"Done! Extracted {cellnumber} cells.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"{newfolderpath} already exists. Skipping.\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f252f79",
   "metadata": {},
   "source": [
    "Access the image files in your folder, which should be placed under Dataset/Structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa97a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./Data/Structures\"\n",
    "\n",
    "try:\n",
    "    folders = os.listdir(directory)\n",
    "except FileNotFoundError:\n",
    "    print(\"No cell image folders found.\")\n",
    "\n",
    "for folder in folders:\n",
    "    folderpath = os.path.join(directory, folder)\n",
    "    generatecellcrops(folderpath, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b78da",
   "metadata": {},
   "source": [
    "## Training our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4099fb0",
   "metadata": {},
   "source": [
    "Our model is finetuned from EfficientNet-LiteB0[2] from timm using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03198dca",
   "metadata": {},
   "source": [
    "Import Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e373c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38e440",
   "metadata": {},
   "source": [
    "Create folders if they do not already exist and split data. 80% of data will be used as training data and 20% as validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08945f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data already exists.\n",
      "./Data/Train already exists.\n",
      "./Data/Validate already exists.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "folders = [\"./Data\", \"./Data/Train\", \"./Data/Validate\"]\n",
    "\n",
    "for folder in folders:\n",
    "    create_folder(folder)\n",
    "\n",
    "directory = \"./Data/Train\"\n",
    "try:\n",
    "    folders = os.listdir(directory) # access all classifications\n",
    "except FileNotFoundError:\n",
    "    print(\"No data folders found.\")\n",
    "\n",
    "for folder in folders:\n",
    "    try:\n",
    "        folderpath = os.path.join(directory, folder)\n",
    "        files = os.listdir(folderpath) # access all images\n",
    "\n",
    "        validatepath = os.path.join(\"./Data/Validate\", folder)\n",
    "        create_folder(validatepath) # create folders in Data Validate\n",
    "\n",
    "        size = len(files)\n",
    "        validate = (size * 2) // 10\n",
    "\n",
    "        for file in files[:validate]: # move the 20% to validate\n",
    "            path = os.path.join(directory, folder, file)\n",
    "            newpath = os.path.join(validatepath, file)\n",
    "            shutil.move(path, newpath)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        continue # skip to the next folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0e6c7",
   "metadata": {},
   "source": [
    "Training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06152430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0.0, 0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_acc = total_correct / len(loader.dataset)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83cd72",
   "metadata": {},
   "source": [
    "Training cycle evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac3053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss    = criterion(outputs, labels)\n",
    "\n",
    "            total_loss    += loss.item() * imgs.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_acc  = total_correct / len(loader.dataset)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12979ac2",
   "metadata": {},
   "source": [
    "The main runner code block to finetune the model. Currently we are using 20 epochs to cut down on processing time, given access to more computing resources, the number of epochs can be increased.\n",
    "\n",
    "Similarly to Image Processing, finetuning will also use the GPU if it is available, it is highly reccomended to use a GPU.\n",
    "\n",
    "Please run the other cells under \"Training our model\" before running this block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8948e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU and CUDA detected, using GPU.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 19/50 [15:54<25:56, 50.22s/it, T_Loss=0.1751, T_Acc=0.9489, V_Loss=0.2466, V_Acc=0.9286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20. No improvement in the last 10 epochs.\n",
      "Classifications.txt not found, creating new file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50 # change number of epochs here\n",
    "\n",
    "# ImageNet normalization statistics\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Process images to tensors for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder('Data/Train', transform=train_transform) # Training on cell crop images\n",
    "val_ds = datasets.ImageFolder('Data/Validate', transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "num_classes = len(train_ds.classes) # number of classifications\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU and CUDA detected, using GPU.\\n\")\n",
    "else:\n",
    "    print(\"GPU and/or CUDA not detected, using CPU.\\n\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loading in EfficientNet-LiteB0\n",
    "model = timm.create_model('efficientnet_lite0', pretrained=True)\n",
    "\n",
    "# Replace the classifier head\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# Load model in CPU/GPU\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience = 10\n",
    "no_improve_epochs = 0\n",
    "pbar = tqdm(total=num_epochs, desc=\"Training\") # Progress bar to estimate training time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "    val_loss, val_acc = eval_epoch(val_loader, model, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"T_Loss\": f\"{train_loss:.4f}\",\n",
    "        \"T_Acc\":  f\"{train_acc:.4f}\",\n",
    "        \"V_Loss\": f\"{val_loss:.4f}\",\n",
    "        \"V_Acc\":  f\"{val_acc:.4f}\"\n",
    "    })\n",
    "\n",
    "    # save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. No improvement in the last {patience} epochs.\")\n",
    "        break\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "try:\n",
    "    os.remove(\"Classifications.txt\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Classifications.txt not found, creating new file.\")\n",
    "    \n",
    "with open(\"Classifications.txt\", \"w\") as file: # write classifications file\n",
    "    file.write(train_ds.classes[0])\n",
    "    for Class in train_ds.classes[1:]:\n",
    "        file.write(f\"\\n{Class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efedcf",
   "metadata": {},
   "source": [
    "Test the finetuned model. Cell images will first be cropped and saved to ./Output/Cells before identification takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0592d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Output already exists.\n",
      "./Output/Cells already exists.\n",
      "Predicted class index of cell 1: Animal_Cell_Mitosis_Cell  (confidence: 63.78%)\n",
      "Predicted class index of cell 2: Motorneurons_Cell  (confidence: 36.88%)\n",
      "Predicted class index of cell 3: Ovary_Of_Lilium_Cell  (confidence: 53.40%)\n",
      "Predicted class index of cell 4: Stomach_Wall_Cell  (confidence: 62.33%)\n",
      "Predicted class index of cell 5: Onion_Root_Tip_Cell  (confidence: 31.35%)\n",
      "Predicted class index of cell 6: Stomach_Wall_Cell  (confidence: 28.89%)\n",
      "Predicted class index of cell 7: Stomach_Wall_Cell  (confidence: 19.98%)\n",
      "Predicted class index of cell 8: Motorneurons_Cell  (confidence: 58.18%)\n",
      "Predicted class index of cell 9: Stomach_Wall_Cell  (confidence: 47.29%)\n",
      "Predicted class index of cell 10: Stomach_Wall_Cell  (confidence: 24.73%)\n",
      "Predicted class index of cell 11: Stomach_Wall_Cell  (confidence: 65.20%)\n",
      "Predicted class index of cell 12: Ovary_Of_Lilium_Cell  (confidence: 36.71%)\n",
      "Predicted class index of cell 13: Onion_Root_Tip_Cell  (confidence: 27.78%)\n",
      "Predicted class index of cell 14: Ovary_Of_Lilium_Cell  (confidence: 34.30%)\n",
      "Predicted class index of cell 15: Stomach_Wall_Cell  (confidence: 41.32%)\n",
      "Predicted class index of cell 16: Motorneurons_Cell  (confidence: 44.13%)\n",
      "Predicted class index of cell 17: Stomach_Wall_Cell  (confidence: 42.61%)\n",
      "Predicted class index of cell 18: Stomach_Wall_Cell  (confidence: 31.87%)\n",
      "Predicted class index of cell 19: Motorneurons_Cell  (confidence: 25.02%)\n",
      "Predicted class index of cell 20: Ovary_Of_Lilium_Cell  (confidence: 45.48%)\n",
      "Predicted class index of cell 21: Stomach_Wall_Cell  (confidence: 84.28%)\n",
      "Predicted class index of cell 22: Motorneurons_Cell  (confidence: 35.10%)\n",
      "Predicted class index of cell 23: Stomach_Wall_Cell  (confidence: 54.00%)\n",
      "Predicted class index of cell 24: Stomach_Wall_Cell  (confidence: 66.60%)\n",
      "Predicted class index of cell 25: Stomach_Wall_Cell  (confidence: 26.66%)\n",
      "Predicted class index of cell 26: Stomach_Wall_Cell  (confidence: 66.89%)\n",
      "Predicted class index of cell 27: Stomach_Wall_Cell  (confidence: 70.29%)\n",
      "Predicted class index of cell 28: Stomach_Wall_Cell  (confidence: 42.41%)\n",
      "Predicted class index of cell 29: Stomach_Wall_Cell  (confidence: 50.14%)\n",
      "Predicted class index of cell 30: Stomach_Wall_Cell  (confidence: 35.84%)\n",
      "Predicted class index of cell 31: Stomach_Wall_Cell  (confidence: 42.75%)\n",
      "Predicted class index of cell 32: Stomach_Wall_Cell  (confidence: 28.49%)\n",
      "Predicted class index of cell 33: Animal_Cell_Mitosis_Cell  (confidence: 66.54%)\n",
      "Predicted class index of cell 34: Stomach_Wall_Cell  (confidence: 49.19%)\n",
      "Predicted class index of cell 35: Stomach_Wall_Cell  (confidence: 28.50%)\n",
      "Predicted class index of cell 36: Animal_Cell_Mitosis_Cell  (confidence: 27.37%)\n",
      "Predicted class index of cell 37: Stomach_Wall_Cell  (confidence: 61.03%)\n",
      "Predicted class index of cell 38: Stomach_Wall_Cell  (confidence: 47.08%)\n",
      "Predicted class index of cell 39: Stomach_Wall_Cell  (confidence: 37.61%)\n",
      "Predicted class index of cell 40: Stomach_Wall_Cell  (confidence: 48.13%)\n",
      "Predicted class index of cell 41: Stomach_Wall_Cell  (confidence: 37.43%)\n",
      "Predicted class index of cell 42: Animal_Cell_Mitosis_Cell  (confidence: 38.77%)\n",
      "Predicted class index of cell 43: Animal_Cell_Mitosis_Cell  (confidence: 43.69%)\n",
      "Predicted class index of cell 44: Stomach_Wall_Cell  (confidence: 58.56%)\n",
      "Predicted class index of cell 45: Stomach_Wall_Cell  (confidence: 50.85%)\n",
      "Predicted class index of cell 46: Stomach_Wall_Cell  (confidence: 66.60%)\n",
      "Predicted class index of cell 47: Stomach_Wall_Cell  (confidence: 28.11%)\n",
      "Predicted class index of cell 48: Animal_Cell_Mitosis_Cell  (confidence: 63.30%)\n",
      "Predicted class index of cell 49: Animal_Cell_Mitosis_Cell  (confidence: 74.85%)\n",
      "Predicted class index of cell 50: Animal_Cell_Mitosis_Cell  (confidence: 76.54%)\n",
      "Predicted class index of cell 51: Animal_Cell_Mitosis_Cell  (confidence: 53.90%)\n",
      "Predicted class index of cell 52: Animal_Cell_Mitosis_Cell  (confidence: 55.39%)\n",
      "Predicted class index of cell 53: Animal_Cell_Mitosis_Cell  (confidence: 90.32%)\n",
      "Predicted class index of cell 54: Stomach_Wall_Cell  (confidence: 52.07%)\n",
      "Predicted class index of cell 55: Stomach_Wall_Cell  (confidence: 70.74%)\n",
      "Predicted class index of cell 56: Stomach_Wall_Cell  (confidence: 71.71%)\n",
      "Predicted class index of cell 57: Stomach_Wall_Cell  (confidence: 38.89%)\n",
      "Predicted class index of cell 58: Cheek_Cells_Cell  (confidence: 46.57%)\n",
      "Predicted class index of cell 59: Stomach_Wall_Cell  (confidence: 20.62%)\n",
      "Predicted class index of cell 60: Ovary_Of_Lilium_Cell  (confidence: 74.18%)\n",
      "Predicted class index of cell 61: Ovary_Of_Lilium_Cell  (confidence: 39.59%)\n",
      "Predicted class index of cell 62: Stomach_Wall_Cell  (confidence: 57.05%)\n",
      "Predicted class index of cell 63: Ovary_Of_Lilium_Cell  (confidence: 77.94%)\n",
      "Predicted class index of cell 64: Stomach_Wall_Cell  (confidence: 30.96%)\n",
      "Predicted class index of cell 65: Animal_Cell_Mitosis_Cell  (confidence: 56.08%)\n",
      "Predicted class index of cell 66: Onion_Root_Tip_Cell  (confidence: 50.75%)\n",
      "Predicted class index of cell 67: Animal_Cell_Mitosis_Cell  (confidence: 63.25%)\n",
      "Predicted class index of cell 68: Stomach_Wall_Cell  (confidence: 50.70%)\n",
      "Predicted class index of cell 69: Ovary_Of_Lilium_Cell  (confidence: 39.80%)\n",
      "Predicted class index of cell 70: Stomach_Wall_Cell  (confidence: 47.32%)\n",
      "Predicted class index of cell 71: Ovary_Of_Lilium_Cell  (confidence: 70.11%)\n",
      "Predicted class index of cell 72: Ovary_Of_Lilium_Cell  (confidence: 51.11%)\n",
      "Predicted class index of cell 73: Animal_Cell_Mitosis_Cell  (confidence: 38.28%)\n",
      "Predicted class index of cell 74: Animal_Cell_Mitosis_Cell  (confidence: 68.46%)\n",
      "Predicted class index of cell 75: Animal_Cell_Mitosis_Cell  (confidence: 43.03%)\n",
      "Predicted class index of cell 76: Animal_Cell_Mitosis_Cell  (confidence: 70.29%)\n",
      "Predicted class index of cell 77: Animal_Cell_Mitosis_Cell  (confidence: 71.93%)\n",
      "Predicted class index of cell 78: Ovary_Of_Lilium_Cell  (confidence: 54.88%)\n",
      "Predicted class index of cell 79: Stomach_Wall_Cell  (confidence: 50.79%)\n",
      "Predicted class index of cell 80: Ovary_Of_Lilium_Cell  (confidence: 24.45%)\n",
      "Predicted class index of cell 81: Stomach_Wall_Cell  (confidence: 33.30%)\n",
      "Predicted class index of cell 82: Stomach_Wall_Cell  (confidence: 25.59%)\n",
      "Predicted class index of cell 83: Ovary_Of_Lilium_Cell  (confidence: 40.36%)\n",
      "Predicted class index of cell 84: Stomach_Wall_Cell  (confidence: 48.78%)\n",
      "Predicted class index of cell 85: Stomach_Wall_Cell  (confidence: 55.75%)\n",
      "Predicted class index of cell 86: Motorneurons_Cell  (confidence: 56.77%)\n",
      "Predicted class index of cell 87: Animal_Cell_Mitosis_Cell  (confidence: 31.61%)\n",
      "Predicted class index of cell 88: Stomach_Wall_Cell  (confidence: 35.18%)\n",
      "Predicted class index of cell 89: Stomach_Wall_Cell  (confidence: 79.75%)\n",
      "Predicted class index of cell 90: Stomach_Wall_Cell  (confidence: 47.59%)\n",
      "Predicted class index of cell 91: Motorneurons_Cell  (confidence: 30.64%)\n",
      "Predicted class index of cell 92: Motorneurons_Cell  (confidence: 41.19%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cellpose\n",
    "from cellpose import models, io\n",
    "from skimage import color, util, measure\n",
    "import numpy as np\n",
    "\n",
    "outputdirectory = \"./Output\"\n",
    "cellcroppath = \"./Output/Cells\"\n",
    "create_folder(outputdirectory)\n",
    "create_folder(cellcroppath)\n",
    "\n",
    "cellpose_model = models.CellposeModel(gpu=True)\n",
    "\n",
    "Classes = []\n",
    "with open(\"Classifications.txt\", \"r\") as file:\n",
    "    Classes = [line.strip() for line in file]\n",
    "num_classes = len(Classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = timm.create_model('efficientnet_lite0', pretrained=False)\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# test image\n",
    "img_path = 'test.jpg'\n",
    "img = io.imread(img_path) # read image file\n",
    "if img.ndim == 2:\n",
    "    img_gray = img\n",
    "else:\n",
    "    img_gray = color.rgb2gray(img) # gray scale image for easier masking\n",
    "inp = (img_gray * 255).astype(np.uint8) # convert from floating-point to 8-bit image\n",
    "\n",
    "masks, flows, styles = cellpose_model.eval(inp) # mask image\n",
    "\n",
    "props = measure.regionprops(masks, intensity_image=img_gray) # get cell mask properties\n",
    "\n",
    "for cellnumber, prop in enumerate(props, 1):\n",
    "    if prop.area < 50 or prop.mean_intensity < 0.1: # remove mask noise pickup from poorly prepared cell samples\n",
    "        continue\n",
    "\n",
    "    label = prop.label\n",
    "    minr, minc, maxr, maxc = prop.bbox # top-left and bottom-right of bounding box\n",
    "\n",
    "    cell_mask = (masks == label) # True when pixels are in current region\n",
    "\n",
    "    cell_img = np.zeros_like(img) # copy the shape of the original image\n",
    "\n",
    "    if img.ndim == 2: # if image is gray scale\n",
    "        cell_img[cell_mask] = img[cell_mask] # copy over the pixels that are inside the region\n",
    "\n",
    "    else: # if image has color\n",
    "        for c in range(img.shape[2]):\n",
    "            channel = img[...,c]\n",
    "            channel_out = np.zeros_like(channel)\n",
    "            channel_out[cell_mask] = channel[cell_mask]\n",
    "            cell_img[...,c] = channel_out\n",
    "\n",
    "    cell_crop = cell_img[minr:maxr, minc:maxc] # crop to bounding box\n",
    "\n",
    "    if cell_crop.dtype != np.uint8: # convert to 8-bit image to save as .png\n",
    "        cell_crop = util.img_as_ubyte(cell_crop)\n",
    "\n",
    "    individualcellcroppath = os.path.join(cellcroppath, f\"cell_{cellnumber}.png\")\n",
    "    io.imsave(individualcellcroppath, cell_crop)\n",
    "\n",
    "    cell_crop = Image.fromarray(cell_crop)\n",
    "        \n",
    "    input_tensor = val_transform(cell_crop).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        pred_class = logits.argmax(dim=1).item()\n",
    "        pred_conf  = probs[0, pred_class].item()\n",
    "\n",
    "    print(f\"Predicted class index of cell {cellnumber}: {Classes[pred_class]}  (confidence: {pred_conf:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a06772",
   "metadata": {},
   "source": [
    "# **Citations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd8b1a",
   "metadata": {},
   "source": [
    "[1] M. Pachitariu, M. Rariden, and C. Stringer, “Cellpose-SAM: superhuman generalization for cellular segmentation,” May 01, 2025, bioRxiv. doi: 10.1101/2025.04.28.651001.\n",
    "\n",
    "[2] M. Tan and Q. V. Le, “EfficientNet: Rethinking model scaling for convolutional neural networks,” arXiv preprint arXiv:1905.11946, 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
